[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a Ph.D. candidate at the Lab for Cognitive and Perceptual Learning at Brown University. My research interests include the mechanisms of visual perceptual learning and how sleep influences the plasticity. I use computational modeling as well as computational imaging to understand the dynamics of plasticity in human visual system with experience. I\u0026rsquo;m also obtaining a doctoral certificate in data science.\nIn my leisure time, I like tennis and snowboarding. I speak English and Mandarin. I also speak some Japanese, Spanish and a little Russian and French.\n","date":1567900800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1567900800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://zhiyanwang27.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a Ph.D. candidate at the Lab for Cognitive and Perceptual Learning at Brown University. My research interests include the mechanisms of visual perceptual learning and how sleep influences the plasticity. I use computational modeling as well as computational imaging to understand the dynamics of plasticity in human visual system with experience. I\u0026rsquo;m also obtaining a doctoral certificate in data science.\nIn my leisure time, I like tennis and snowboarding. I speak English and Mandarin.","tags":null,"title":"Zhiyan Wang","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1575550800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575550800,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://zhiyanwang27.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"TBD","tags":[],"title":"Invited Talk","type":"talk"},{"authors":["Masako Tamaki","Zhiyan Wang","Tyler Barnes-Diana","Aaron V Berard","Edward Walsh","Takeo Watanabe","Yuka Sasaki"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1567900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567900800,"objectID":"10da286ea2d970beadf9436ed79b14f4","permalink":"https://zhiyanwang27.github.io/publication/coauthor/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/coauthor/","section":"publication","summary":"Sleep is beneficial for learning. However, whether NREM or REM sleep facilitates learning, whether the learning facilitation results from plasticity increases or stabilization and whether the facilitation results from learning-specific processing are all controversial. Here, after training on a visual task we measured the excitatory and inhibitory neurochemical (E/I) balance, an index of plasticity in human visual areas, for the first time, while subjects slept. Off-line performance gains of presleep learning were associated with the E/I balance increase during NREM sleep, which also occurred without presleep training. In contrast, increased stabilization was associated with decreased E/I balance during REM sleep only after presleep training. These indicate that the above-mentioned issues are not matters of controversy but reflect opposite neurochemical processing for different roles in learning during different sleep stages: NREM sleep increases plasticity leading to performance gains independently of learning, while REM sleep decreases plasticity to stabilize learning in a learning-specific manner.","tags":["Source Themes"],"title":"Opponent neurochemical and functional processing in NREM and REM sleep in visual learning","type":"publication"},{"authors":["Zhiyan Wang","Dongho Kim","Giorgia Pedroncelli","Yuka Sasaki","Takeo Watanabe"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1567900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567900800,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://zhiyanwang27.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Visual perceptual learning (VPL) is defined as a long-term performance enhancement as a result of visual experiences. A number of studies have demonstrated that reward can evoke VPL. However, the mechanisms of how reward evoke VPL remain unknown. One possible hypothesis is that VPL is obtained through reward related reinforcement processing. If this hypothesis is true, learning can only occur when reward follows the stimulus presentation. Another interpretation is that VPL is acquired through an enhancement of alertness in association with reward. If the alertness hypothesis is true, learning should occur when reward precedes the stimulus presentation. In our study, we tested the plausibility of the two hypotheses by manipulating the order of reward and stimulus presentation. In Experiment 1, we separated participants into two groups. During training, the ‘Before’ group received water reward 400ms prior to the onset of trained orientation stimulus while the ‘After’ group received water reward 400ms subsequent to the onset of trained orientation stimulus. Both groups were trained using the Continuous Flash Suppression paradigm to render the stimulus imperceptible to the participants by the presentation of dynamic noise in the untrained eye. We found training only in the ‘After’ group indicating that reward may evoke learning through reinforcement-like processing. In Experiment 2, we excluded the possibility that alertness may not be sufficient to elicit learning when presented before stimulus. We presented beep sound prior to the onset of stimulus to increase alertness. Our finding demonstrated that alertness is sufficient enough to evoke learning. In conclusion, our study provided evidence that reward can evoke VPL through reinforcement process.","tags":["Source Themes"],"title":"Reward Evokes Visual Perceptual Learning Following Reinforcement Learning Rules","type":"publication"},{"authors":["Qingleng Tan","Zhiyan Wang","Yuka Sasaki","Takeo Watanabe"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1555891200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555891200,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://zhiyanwang27.github.io/publication/journal-article/","publishdate":"2019-04-22T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Visual perceptual learning (VPL) refers to a long-term enhancement of visual task performance as a result of visual experience. VPL is generally specific for the trained visual feature, meaning that training on a feature leads to performance enhancement only on the feature and those in its close vicinity. In the meantime, visual perception is often categorical. This may partially be because the ecological importance of a stimulus is usually determined by the category to which the stimulus belongs (e.g., snake, lightning, and fish). Thus, it would be advantageous to an observer if encountering or working on a feature from a category increases sensitivity to features under the same category. However, studies of VPL have used uncategorized features. Here, we found a category-induced transfer of VPL, where VPL of an orientation transferred to untrained orientations within the same category as the trained orientation, but not orientations from the different category. Furthermore, we found that, although category learning transferred to other locations in the visual field, the category-induced transfer of VPL occurred only when visual stimuli for the category learning and those for VPL training were presented at the same location. These results altogether suggest that feature specificity in VPL is greatly influenced by cognitive processing, such as categorization in a top-down fashion. In an environment where features are categorically organized, VPL may be more generalized across features under the same category. Such generalization implies that VPL is of more ecological significance than has been thought.","tags":["Source Themes"],"title":"Category-Induced Transfer of Visual Perceptual Learning","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://zhiyanwang27.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Masako Tamaki","Zhiyan Wang","Takeo Watanabe","Yuka Sasaki"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1538092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538092800,"objectID":"c6c3c7f6797d61a186452ae648f2a565","permalink":"https://zhiyanwang27.github.io/publication/jov/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/jov/","section":"publication","summary":"It has been suggested that sleep provides additional enhancement of visual perceptual learning (VPL) acquired before sleep, termed offline performance gains. A majority of the studies that found offline performance gains of VPL used discrimination tasks including the texture discrimination task (TDT). This makes it questionable whether offline performance gains on VPL are generalized to other visual tasks. The present study examined whether a Gabor orientation detection task, which is a standard task in VPL, shows offline performance gains. In Experiment 1, we investigated whether sleep leads to offline performance gains on the task. Subjects were trained with the Gabor orientation detection task, and re-tested it after a 12-hr interval that included either nightly sleep or only wakefulness. We found that performance on the task improved to a significantly greater degree after the interval that included sleep and wakefulness than the interval including wakefulness alone. In addition, offline performance gains were specific to the trained orientation. In Experiment 2, we tested whether offline performance gains occur by a nap. Also, we tested whether spontaneous sigma activity in early visual areas during non-rapid eye movement (NREM) sleep, previously implicated in offline performance gains of TDT, was associated with offline performance gains of the task. A different group of subjects had a nap with polysomnography. The subjects were trained with the task before the nap and re-tested after the nap. The performance of the task improved significantly after the nap only on the trained orientation. Sigma activity in the trained region of early visual areas during NREM sleep was significantly larger than in the untrained region, in correlation with offline performance gains. These aspects were also found with VPL of TDT. The results of the present study demonstrate that offline performance gains are not specific to a discrimination task such as TDT, and can be generalized to other forms of VPL tasks, along with trained-feature specificity. Moreover, the present results also suggest that sigma activity in the trained region of early visual areas plays an important role in offline performance gains of VPL of detection as well as discrimination tasks.","tags":["Source Themes"],"title":"Trained-feature specific offline learning in an orientation detection task","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://zhiyanwang27.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://zhiyanwang27.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"268680cff01e377af0e1ea3450ad65fd","permalink":"https://zhiyanwang27.github.io/post/_index-2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/_index-2/","section":"post","summary":"","tags":null,"title":"Posts","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ae50579bf4c3979046d5ba4bc1359c6e","permalink":"https://zhiyanwang27.github.io/publication/_index-2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/_index-2/","section":"publication","summary":"","tags":null,"title":"Publications","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ede4de32a6c8d27a3e509d79573b9008","permalink":"https://zhiyanwang27.github.io/talk/_index-2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/_index-2/","section":"talk","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"talk"}]